{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SMQd2_1rPxC",
        "outputId": "dca6e5a7-6cfa-446d-db31-fb87f4127455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor(18.9843), tensor(45.6913), tensor(67.4386), tensor(35.9074), tensor(53.2924), tensor(40.2480), tensor(31.6699), tensor(28.4670), tensor(51.0383), tensor(43.0060), tensor(62.8962), tensor(48.9364), tensor(50.9113), tensor(58.8567), tensor(39.8095), tensor(39.7580), tensor(39.3271), tensor(41.9701), tensor(47.1500), tensor(31.8783), tensor(43.8330), tensor(52.6606), tensor(53.5816), tensor(35.8898), tensor(33.3435), tensor(49.0478), tensor(51.5003), tensor(29.4431), tensor(32.9550), tensor(61.4541), tensor(43.9555), tensor(40.9688), tensor(43.1617), tensor(61.3024), tensor(64.3761), tensor(56.4112), tensor(30.0364), tensor(18.5321), tensor(50.1029), tensor(27.8650), tensor(56.9830), tensor(44.8925), tensor(27.1997), tensor(49.6243), tensor(36.2814), tensor(36.2884), tensor(46.9478), tensor(37.8120), tensor(21.9484), tensor(48.2271), tensor(42.6665), tensor(37.3368), tensor(44.3244), tensor(37.9712), tensor(60.6359), tensor(37.4707), tensor(45.6898), tensor(39.4140), tensor(46.0186), tensor(60.9262), tensor(46.1522), tensor(49.8687), tensor(39.0482), tensor(42.7974), tensor(58.6809), tensor(34.3956), tensor(63.5163), tensor(44.4174), tensor(54.1547), tensor(40.6312), tensor(33.8756), tensor(67.3300), tensor(65.0920), tensor(48.8637), tensor(42.3410), tensor(59.9622), tensor(30.2443), tensor(45.7923), tensor(39.3862), tensor(34.9661), tensor(44.6744), tensor(52.7244), tensor(55.6487), tensor(50.5268), tensor(38.0102), tensor(63.7488), tensor(37.8319), tensor(27.9499), tensor(71.1150), tensor(44.6913), tensor(23.1961), tensor(42.9281), tensor(35.7485), tensor(26.9345), tensor(42.2693), tensor(52.6557), tensor(51.0063), tensor(23.4527), tensor(52.4796), tensor(35.3285), tensor(44.1733), tensor(46.2513), tensor(65.0534), tensor(34.0049), tensor(48.1235), tensor(40.4275), tensor(48.1005), tensor(67.5374), tensor(44.5866), tensor(68.5889), tensor(32.3158), tensor(38.4618), tensor(45.5252), tensor(35.6955), tensor(37.8187), tensor(65.8663), tensor(46.4711), tensor(39.7419), tensor(42.3210), tensor(45.3920), tensor(50.3198), tensor(36.3456), tensor(29.6310), tensor(47.2085), tensor(34.1407), tensor(40.0291), tensor(56.7987), tensor(53.9522), tensor(36.5023), tensor(27.1890), tensor(35.6108), tensor(51.1932), tensor(33.0274), tensor(32.0079), tensor(60.1988), tensor(32.1731), tensor(42.0746), tensor(80.8737), tensor(72.2188), tensor(34.9415), tensor(48.2465), tensor(39.1383), tensor(36.4251), tensor(39.4988), tensor(42.8385), tensor(36.7009), tensor(53.3548), tensor(45.1118), tensor(53.7619), tensor(52.1472), tensor(39.3109), tensor(49.9763), tensor(65.7863), tensor(43.9796), tensor(50.9583), tensor(59.5724), tensor(48.2186), tensor(47.1126), tensor(32.1904), tensor(47.1796), tensor(37.9492), tensor(62.3947), tensor(22.9135), tensor(26.3073), tensor(46.9046), tensor(37.6406), tensor(43.5667), tensor(62.1891), tensor(37.1511), tensor(35.6031), tensor(51.4692), tensor(47.6934), tensor(39.9852), tensor(43.6702), tensor(57.7137), tensor(49.2056), tensor(44.9073), tensor(72.7492), tensor(49.2647), tensor(17.4542), tensor(35.0529), tensor(53.5624), tensor(40.0314), tensor(40.5310), tensor(48.4780), tensor(36.1080), tensor(50.2524), tensor(34.7229), tensor(43.3895), tensor(46.1111), tensor(42.3080), tensor(56.0033), tensor(48.0252), tensor(45.4966), tensor(34.8645), tensor(41.5440), tensor(39.8140), tensor(41.3083), tensor(46.6604), tensor(31.8301), tensor(33.0045), tensor(28.7536), tensor(47.7642), tensor(38.2160), tensor(31.9308), tensor(36.1554), tensor(28.7542), tensor(43.3045), tensor(62.0684), tensor(25.4688), tensor(34.7213), tensor(47.8380), tensor(51.7123), tensor(46.8261), tensor(72.4795), tensor(49.3205), tensor(36.4728), tensor(71.9183), tensor(20.0960), tensor(50.0939), tensor(40.8979), tensor(44.3374), tensor(34.3560), tensor(46.6359), tensor(32.3808), tensor(51.8312), tensor(36.7409), tensor(41.2474), tensor(51.7127), tensor(64.4847), tensor(40.3198), tensor(32.1381), tensor(22.3366), tensor(35.0617), tensor(35.7445), tensor(35.4967), tensor(39.8282), tensor(54.2248), tensor(43.4574), tensor(37.5992), tensor(60.0093), tensor(38.5694), tensor(40.5751), tensor(44.7658), tensor(61.0543), tensor(49.0644), tensor(35.4682), tensor(41.7063), tensor(28.2839), tensor(42.6977), tensor(40.3657), tensor(30.0529), tensor(40.8488), tensor(41.8123), tensor(44.9434), tensor(47.0411), tensor(55.1748), tensor(40.0999), tensor(48.3949), tensor(37.1872), tensor(44.3562), tensor(50.1383), tensor(54.1329), tensor(56.6944), tensor(28.0365), tensor(45.9792), tensor(45.4349), tensor(32.5385), tensor(60.5990), tensor(32.2924), tensor(58.6910), tensor(64.5024), tensor(32.1146), tensor(46.8598), tensor(43.1208), tensor(43.4324), tensor(57.9497), tensor(41.7553), tensor(24.9624), tensor(21.4428), tensor(38.7879), tensor(65.0560), tensor(29.0060), tensor(54.6089), tensor(29.9248), tensor(31.9112), tensor(31.3368), tensor(48.7608), tensor(43.0778), tensor(45.8594), tensor(22.0187), tensor(38.6626), tensor(30.0666), tensor(33.7049), tensor(76.6101), tensor(38.6102), tensor(65.7704), tensor(29.3212), tensor(39.7545), tensor(38.4113), tensor(68.3682), tensor(40.3486), tensor(35.6025), tensor(43.7724), tensor(63.4896), tensor(62.9691), tensor(40.3455), tensor(55.6081), tensor(43.2974), tensor(58.9615), tensor(53.8103), tensor(53.8733), tensor(70.5403)]\n",
            "Highest Hessian value: 80.873726\n",
            "Lowest Hessian value: 17.454187\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import grad, Variable\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from copy import deepcopy\n",
        "from typing import Tuple, Union\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Define the simple convolutional network\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc = nn.Linear(32 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Load the MNIST dataset\n",
        "batch_size = 32\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "mnist_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define a function to calculate the highest Hessian value\n",
        "def calculate_highest_hessian(model, loss_fn, dataset):\n",
        "    hessians = []\n",
        "\n",
        "    # Iterate over the dataset\n",
        "    for x, y in dataset:\n",
        "        logit = model(x)\n",
        "        loss = loss_fn(logit, y)\n",
        "        grads_1st = torch.autograd.grad(loss, model.parameters())\n",
        "\n",
        "        frz_model_params = deepcopy(model.state_dict())\n",
        "        delta = 1e-3\n",
        "        dummy_model_params_1 = OrderedDict()\n",
        "        dummy_model_params_2 = OrderedDict()\n",
        "        with torch.no_grad():\n",
        "            for (layer_name, param), grad in zip(model.named_parameters(), grads_1st):\n",
        "                dummy_model_params_1.update({layer_name: param + delta * grad})\n",
        "                dummy_model_params_2.update({layer_name: param - delta * grad})\n",
        "\n",
        "        model.load_state_dict(dummy_model_params_1, strict=False)\n",
        "        logit_1 = model(x)\n",
        "        loss_1 = loss_fn(logit_1, y)\n",
        "        grads_1 = torch.autograd.grad(loss_1, model.parameters())\n",
        "\n",
        "        model.load_state_dict(dummy_model_params_2, strict=False)\n",
        "        logit_2 = model(x)\n",
        "        loss_2 = loss_fn(logit_2, y)\n",
        "        grads_2 = torch.autograd.grad(loss_2, model.parameters())\n",
        "\n",
        "        model.load_state_dict(frz_model_params)\n",
        "\n",
        "        grads = []\n",
        "        with torch.no_grad():\n",
        "            for g1, g2 in zip(grads_1, grads_2):\n",
        "                grads.append((g1 - g2) / (2 * delta))\n",
        "\n",
        "        # Compute the norm for each gradient individually\n",
        "        norms = [torch.norm(grad) for grad in grads]\n",
        "\n",
        "        # Compute the norm over all the norms\n",
        "        norm = torch.norm(torch.stack(norms))\n",
        "\n",
        "        hessians.append(norm*0.9)\n",
        "\n",
        "    return hessians\n",
        "\n",
        "# Create an instance of the ResNet-9 model\n",
        "model = ConvNet()\n",
        "\n",
        "# Calculate the highest Hessian value for the loss function in MNIST\n",
        "hessian_values = calculate_highest_hessian(model, loss_fn, test_loader)\n",
        "print(hessian_values)\n",
        "# Print the highest Hessian value\n",
        "max_hessian_value = np.max(hessian_values)\n",
        "print(\"Highest Hessian value:\", max_hessian_value)\n",
        "min_hessian_value = np.min(hessian_values)\n",
        "print(\"Lowest Hessian value:\", min_hessian_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_list = []\n",
        "eig_list = []\n",
        "B_list = []\n",
        "for i in range(100):\n",
        "    A = np.random.rand(200,200)\n",
        "    eig_val, eig_vec = np.linalg.eig(A)\n",
        "    B = np.linalg.norm(A)\n",
        "    eig_list.append(np.absolute(np.max(eig_val)))\n",
        "    B_list.append(B)\n",
        "    max_list.append(np.absolute(np.max(eig_val))-B)\n",
        "\n",
        "def calculate_average(lst):\n",
        "    total = sum(lst)\n",
        "    average = total / len(lst)\n",
        "    return average\n",
        "\n",
        "average_max = calculate_average(max_list)\n",
        "average_norm = calculate_average(B_list)\n",
        "average_eig = calculate_average(eig_list)\n",
        "print(f\"{average_eig} - {average_norm} - {average_max}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCCPi0aOfAX8",
        "outputId": "247e41e5-9e2b-446d-d45c-a3ce9581fd3e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99.97033244955503 - 115.44230402029002 - -15.471971570734993\n"
          ]
        }
      ]
    }
  ]
}